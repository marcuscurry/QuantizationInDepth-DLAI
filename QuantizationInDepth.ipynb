{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "429630257d9310ce",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Quantization in Depth\n",
    "The goal of this notebook is to provide an in depth understanding of Quantization, its theory, use cases, and implementation. The concepts, and instruction within this notebook are used from the [DeepLearning.AI](https://www.deeplearning.ai) course: [Quantization in Depth](https://learn.deeplearning.ai/courses/quantization-in-depth/lesson/1/introduction). I highly recommend watching, and completing this course on your own time. However, I wanted to provide an all-in-one notebook, including my insights as I take the course for those who may be interested.\n",
    "\n",
    "***\n",
    "## Outline:\n",
    "### Linear Quantization\n",
    "This Notebook aims to help gain an understanding of Linear Quantization. By deep diving into the internals of linear quantization and implementing the variance from scratch (per channel, tensor, and group quantization) we should be able to study the advantages/drawbacks for each method and their impacts on some example tensors.\n",
    "<br><br>\n",
    "### BYO 8-Bit Quantizer\n",
    "Building our own quantizer to quantize any model in 8-bit precision using one of the quantization schemes presented before. Quantization schemes are agnostic to modality, meaning: it can be applied to any model as long as it contains linear layers. technically your quantizer will be able to quantize a vision, text, audio, or even a multimodal model.\n",
    "\n",
    "### Quantization Packages\n",
    "Learn more about challenges that can be faced regarding extreme quantization such as weight packing or challenges regarding LLM quantization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df760b0e92b66dd5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# <font color=orange>Linear Quantization I-A: Quantize and De-quantize a Tensor</font>\n",
    "In this lesson, you will learn the fundamentals of linear quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c66e355208fe3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from helper import plot_quantization_errors, plot_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb9c09ef21b9858",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Quantization with Random Scale and Zero Point\n",
    "* Implement Linear Quantization for when the \"scale\" and the \"zero point\" are known/randomly selected. <br>\n",
    "\n",
    "***Linear Quantization Formula:***\n",
    "**<font color=orange>r</font>** = original value (input/high-bit), **<font color=purple>s</font>** = Scale (input/high-bit)\n",
    "**<font color=red>q</font>** = quantized value (output/Low-bit), **<font color=olive>z</font>** = zero point\n",
    "$ r = s(q - z) $ or $ q = int(round(r/s + z)) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d8f4ee3f44341",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linear_q_with_scale_and_zero_point(tensor, scale, zero_point, dtype = torch.int8):\n",
    "\n",
    "    scaled_and_shifted_tensor = tensor / scale + zero_point\n",
    "\n",
    "    rounded_tensor = torch.round(scaled_and_shifted_tensor)\n",
    "\n",
    "    q_min = torch.iinfo(dtype).min\n",
    "    q_max = torch.iinfo(dtype).max\n",
    "\n",
    "    q_tensor = rounded_tensor.clamp(q_min,q_max).to(dtype)\n",
    "    \n",
    "    return q_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7abd1c3e4a3b5e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_tensor=torch.tensor(\n",
    "    [[191.6, -13.5, 728.6],\n",
    "     [92.14, 295.5,  -184],\n",
    "     [0,     684.6, 245.5]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4470b7c0ccaed9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### these are random values for \"scale\" and \"zero_point\"\n",
    "### to test the implementation\n",
    "scale = 3.5\n",
    "zero_point = -70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d1856fe10a5737",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "quantized_tensor = linear_q_with_scale_and_zero_point(test_tensor, scale, zero_point)\n",
    "quantized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1672a41382a0cbe2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What happens if we do not cast the quantized tensor to float?\n",
    "dequantized_tensor = scale * (quantized_tensor - zero_point)\n",
    "dequantized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3a76c30201ced3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CORRECT implementation: casting quantized tensor to float\n",
    "def linear_dequantization(quantized_tensor, scale, zero_point):\n",
    "    return scale * (quantized_tensor.float() - zero_point)\n",
    "\n",
    "dequantized_tensor = linear_dequantization(quantized_tensor, scale, zero_point)\n",
    "dequantized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa41f4556646dfb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_quantization_errors(test_tensor, quantized_tensor, dequantized_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eb6c031a2caa91",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Quantization Error: Calculate an \"overall\" quantization error by using Mean Squared Error technique.\n",
    "(dequantized_tensor - test_tensor).square().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdd7cd11c8b928e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def quantization_mse(dequantized_tensor, tensor):\n",
    "    print(f\"Quantization Mean Squared Error: {(dequantized_tensor - tensor).square().mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0bdafc09439587",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_error = quantization_mse(dequantized_tensor, test_tensor)\n",
    "q_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2279684f1dafbf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "***Quantization Error: <font color=red>170.8753</font>***\n",
    "This error is considered to be quite high however this is due to the randomly assigned zero-point and scale values. In the next sections we'll cover how to derive closer or exact values\n",
    "<br>\n",
    "#### Advantages of Quantization\n",
    "* Smaller Model\n",
    "* Speed Increase:\n",
    "  * Memory Bandwidth\n",
    "  * Faster Operations:\n",
    "      * GEMM: General Matrix Multiply (matrix to matrix multiplication)\n",
    "      * GEMV: General Matrix Multiply (matrix to vector multiplication)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80677ccc5831640",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# <font color=orange>Linear Quantization I-B: Get the Scale and Zero Point</font>\n",
    "In this lesson, continue to learn about fundamentals of linear quantization, and implement your own Linear Quantizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830ea2b928094e8e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### ***Scale and Zero-Point***\n",
    "If we look at extreme values we should get:\n",
    "$ r_{min} = s(q_{min}- z) $ \n",
    "$ r_{max} = s(q_{max}- z) $ \n",
    "\n",
    "subtracting the first equation from the second we get the scale s:\n",
    "$ s = (r_{max} - r_{min}) / (q_{max} - q_{min})$\n",
    "\n",
    "for the zero point we need to round the value:\n",
    "$ z = int(round(q_{min} - (r_{min}/s))) $\n",
    "*The goal is to represent 0 in the original 'r' range with an integer in the quantized 'q' range*\n",
    "\n",
    "Therefore, for our previous example:\n",
    "$ s = (728.6 - (-184)) / (127 - (-128)) >> 912/255 >>s = 3.58$\n",
    "$ z = int(round((-128) - (-184)/3.58) >> int(round((-128) - (-51.4))) >> int(round(-76.6)) >> z = -77$\n",
    "\n",
    "What do you do if the zero point is out of range?\n",
    "case 1: (z < q_min) >> set z = q_min\n",
    "case 2: (z > q_max) >> set z = q_max\n",
    "*This elminiates overflow and underflow*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85246094aa3cd663",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_min = torch.iinfo(torch.int8).min\n",
    "q_max = torch.iinfo(torch.int8).max\n",
    "print(f\"q_min: {q_min}, q_max: {q_max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c324acaa8aeb04",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r_min = test_tensor.min().item()\n",
    "r_max = test_tensor.max().item()\n",
    "print(f\"r_min: {r_min}, r_max: {r_max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880c1b82bddf1c13",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scale = (r_max - r_min) / (q_max - q_min)\n",
    "print(f\"scale: {scale}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884506fb9a96449b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero_point = int(round(q_min - (r_min/scale)))\n",
    "print(f\"zero_point: {zero_point}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d228e267aac1ad96",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_q_scale_and_zero_point(tensor, dtype=torch.int8):\n",
    "    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n",
    "    r_min, r_max = tensor.min().item(), tensor.max().item()\n",
    "    scale = (r_max - r_min) / (q_max - q_min)\n",
    "    zero_point = (q_min - (r_min/scale))\n",
    "    if zero_point < q_min:\n",
    "        zero_point = q_min\n",
    "    elif zero_point > q_max:\n",
    "        zero_point = q_max\n",
    "    else:\n",
    "        zero_point = int(round(zero_point))\n",
    "        \n",
    "    return scale, zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809fe855bf642fd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_scale , new_zero_point = get_q_scale_and_zero_point(test_tensor)\n",
    "print(f\"new_scale: {new_scale}, new_zero_point: {new_zero_point}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e4759b1f62a9e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "quantized_tensor = linear_q_with_scale_and_zero_point(test_tensor, new_scale, new_zero_point)\n",
    "dequantized_tensor = linear_dequantization(quantized_tensor, new_scale, new_zero_point)\n",
    "plot_quantization_errors(test_tensor, quantized_tensor,dequantized_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9fc2e39c2d82dc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(dequantized_tensor - test_tensor).square().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786cfb36a488e28d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linear_quantization(tensor, dtype=torch.int8):\n",
    "    scale, zero_point = get_q_scale_and_zero_point(tensor, dtype=dtype)\n",
    "    quantized_tensor = linear_q_with_scale_and_zero_point(tensor, scale, zero_point, dtype=dtype)\n",
    "    return quantized_tensor, scale, zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69266c3b1a3da84",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r_tensor = torch.randn((4,4))\n",
    "r_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffcc58c6349d491",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "quantized_tensor, scale, zero_point = linear_quantization(r_tensor)\n",
    "quantized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e9872781c7453",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56533c0576bf01d0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bfcc24ddda45f7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dequantized_tensor = linear_dequantization(quantized_tensor, scale, zero_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d2e7d90c7efbee",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_quantization_errors(r_tensor, quantized_tensor, dequantized_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7031a94fcc2d832",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "quantization_mse(dequantized_tensor, r_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed8f7213a5c5a83",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Custom Attempt at Batch Normalization Quantization\n",
    "Turns out specifically, this class uses per channel quantization (per row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b00c7d68bbf874",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BatchTensor:\n",
    "    def __init__(self, tensor, dtype):\n",
    "        # TENSORS\n",
    "        self.tensor = tensor\n",
    "        self.quantized_tensor = None\n",
    "        self.dequantized_tensor = None\n",
    "        \n",
    "        # VARIABLES\n",
    "        self.dtype = dtype\n",
    "        self.rmin = self.tensor.min().item()\n",
    "        self.rmax = self.tensor.max().item()\n",
    "        self.qmin = torch.iinfo(self.dtype).min\n",
    "        self.qmax = torch.iinfo(self.dtype).max\n",
    "        self.scale = []\n",
    "        self.zero_point = []\n",
    "        \n",
    "        # MISCELLANEOUS\n",
    "        self.rows_size = tensor.size(0)\n",
    "        self.cols_size = tensor.size(1)\n",
    "        self.mse_tensor = None\n",
    "        \n",
    "    def get_batch_scales_and_zeropoints(self):\n",
    "        if self.scale or self.zero_point:\n",
    "            self.clear()\n",
    "        for i in range(self.rows_size):\n",
    "            a, b = get_q_scale_and_zero_point(self.tensor[i])\n",
    "            self.scale.append(a), self.zero_point.append(b)\n",
    "            \n",
    "    def linear_batch_quantization(self):\n",
    "        self.quantized_tensor = torch.zeros(self.rows_size, self.cols_size)\n",
    "        for i in range(self.rows_size):\n",
    "            self.quantized_tensor[i] = linear_q_with_scale_and_zero_point(self.tensor[i], self.scale[i], self.zero_point[i])\n",
    "    \n",
    "    def linear_batch_dequantization(self):\n",
    "        self.dequantized_tensor = torch.zeros(self.rows_size, self.cols_size)\n",
    "        for i in range(self.rows_size):\n",
    "            self.dequantized_tensor[i] = linear_dequantization(self.quantized_tensor[i], self.scale[i], self.zero_point[i])\n",
    "            \n",
    "    def batch_quantization_mse(self):\n",
    "        return (bt.dequantized_tensor - bt.tensor).square().mean()\n",
    "            \n",
    "    def clear(self):\n",
    "        self.scale.clear()\n",
    "        self.zero_point.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b360c18f2ca71b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = torch.randn((4,4))\n",
    "bt = BatchTensor(t, torch.int8)\n",
    "bt.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a24a8cf6d098202",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bt.get_batch_scales_and_zeropoints()\n",
    "bt.linear_batch_quantization()\n",
    "bt.linear_batch_dequantization()\n",
    "plot_quantization_errors(bt.tensor, bt.quantized_tensor, bt.dequantized_tensor)\n",
    "bt.batch_quantization_mse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c11c52d119491b9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_scale , new_zero_point = get_q_scale_and_zero_point(bt.tensor)\n",
    "quantized_tensor = linear_q_with_scale_and_zero_point(bt.tensor, new_scale, new_zero_point)\n",
    "dequantized_tensor = linear_dequantization(quantized_tensor, new_scale, new_zero_point)\n",
    "plot_quantization_errors(bt.tensor, quantized_tensor,dequantized_tensor)\n",
    "quantization_mse(dequantized_tensor, bt.tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d517d1244ed0c1e0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# <font color=orange>Linear Quantization II-A: Symmetric vs. Asymmetric Mode</font>\n",
    "In this lesson, you will learn a different way of performing linear quantization, Symmetric Mode.\n",
    "\n",
    "There are **two** modes in linear quantization\n",
    "* **Asymmetric**: We map [$r_{min}, r_{max}$] to [$q_{min}, q_{max}$] (*What was implemented previously*)\n",
    "* **Symmetric**: We map [$-r_{min}, r_{max}$] to [$-q_{min}, q_{max}$], where we can set $r_{max} = max(|r_{tensor}|)$\n",
    "    * We don't need to use zero point ($z=0$), beacuse the floating point range and the quantized range are symmetric with respect to zero\n",
    "    * Hence, We can simplify the equations to:\n",
    "    * $q = int(round(r/s))$\n",
    "    * $s = r_{max}/q_{max}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c1dd27734c59a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from helper import plot_quantization_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46358455744e469",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_q_scale_symmetric(tensor, dtype=torch.int8):\n",
    "    r_max = tensor.abs().max().item()\n",
    "    q_max = torch.iinfo(dtype).max\n",
    "    return r_max / q_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cca837aeb1a24c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_tensor = torch.rand((4,4))\n",
    "test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde979e506c1c1c2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_q_scale_symmetric(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed89dbf9446b683",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linear_q_symmetric(tensor, dtype=torch.int8):\n",
    "    scale = get_q_scale_symmetric(tensor)\n",
    "    quantized_tensor = linear_q_with_scale_and_zero_point(tensor, scale=scale, zero_point=0, dtype=dtype)\n",
    "    return quantized_tensor, scale    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81696827ff194411",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "quantized_tensor, scale = linear_q_symmetric(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e335aa7713b77a3a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dequantized_tensor = linear_dequantization(quantized_tensor, scale, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba861501421df98",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_quantization_errors(test_tensor, quantized_tensor, dequantized_tensor)\n",
    "quantization_mse(dequantized_tensor, test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89017e9de2d69a86",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Trade-Offs:\n",
    "* **Utilization of Quantized Range**\n",
    "    * Asymmetric quantization fully utilizes the quantized range\n",
    "    * Symmetric mode will dedicate values of the quantized range to unnecessary values if the float range is biased towards one side. (e.g. RELU where the output is always positive)\n",
    "*  **Simplicity**\n",
    "    * Symmetric mode is much simpler and straightforward then Assymetric mode\n",
    "*  **Memory**\n",
    "    * Zero-points are not stored which saves memory    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7ca07ab9b44969",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# <font color=orange>Linear Quantization II-B: Finer Granularity for more Precision</font>\n",
    "In this lesson, you will learn about different granularities of performing linear quantization.\n",
    "\n",
    "### Per Tensor Quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4164722b84946b5e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_tensor=torch.tensor(\n",
    "    [[191.6, -13.5, 728.6],\n",
    "     [92.14, 295.5,  -184],\n",
    "     [0,     684.6, 245.5]]\n",
    ")\n",
    "\n",
    "quantized_tensor, scale = linear_q_symmetric(test_tensor)\n",
    "dequantized_tensor = linear_dequantization(quantized_tensor, scale, 0)\n",
    "plot_quantization_errors(test_tensor, quantized_tensor, dequantized_tensor)\n",
    "quantization_mse(dequantized_tensor, test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1829abc23848bd6a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Per Channel Quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328c2c1785b27203",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_tensor=torch.tensor(\n",
    "    [[191.6, -13.5, 728.6],\n",
    "     [92.14, 295.5,  -184],\n",
    "     [0,     684.6, 245.5]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca36577d32786b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim=0 # dim=0 means along rows, dim=1 means along columns\n",
    "output_dim = test_tensor.shape[dim]\n",
    "output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b19cf9210b2a3ea",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scale = torch.zeros(output_dim)\n",
    "scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f68733ecbd8f56",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(output_dim):\n",
    "    sub_tensor = test_tensor.select(dim, i)\n",
    "    scale[i] = get_q_scale_symmetric(sub_tensor)\n",
    "scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9ddd1b935d22c8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scale_shape = [1] * test_tensor.dim()\n",
    "scale_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20095dfaf4cc9c3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scale_shape[dim]=-1\n",
    "scale_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd3cb23a8e21671",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scale = scale.view(scale_shape)\n",
    "scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bd30c2cd5b6aff",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1610d24c0e242d6a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "quantized_tensor = linear_q_with_scale_and_zero_point(test_tensor, scale=scale, zero_point=0)\n",
    "quantized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b719dd307f1781d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linear_q_symmetric_per_channel(r_tensor, dim, dtype=torch.int8):\n",
    "    \n",
    "    output_dim = r_tensor.shape[dim]\n",
    "    # store the scales\n",
    "    scale = torch.zeros(output_dim)\n",
    "\n",
    "    for index in range(output_dim):\n",
    "        sub_tensor = r_tensor.select(dim, index)\n",
    "        scale[index] = get_q_scale_symmetric(sub_tensor, dtype=dtype)\n",
    "\n",
    "    # reshape the scale\n",
    "    scale_shape = [1] * r_tensor.dim()\n",
    "    scale_shape[dim] = -1\n",
    "    scale = scale.view(scale_shape)\n",
    "    quantized_tensor = linear_q_with_scale_and_zero_point(\n",
    "        r_tensor, scale=scale, zero_point=0, dtype=dtype)\n",
    "   \n",
    "    return quantized_tensor, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b63afa9afd9057",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_tensor=torch.tensor(\n",
    "    [[191.6, -13.5, 728.6],\n",
    "     [92.14, 295.5,  -184],\n",
    "     [0,     684.6, 245.5]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef638a23491c6dee",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### along the rows (dim = 0)\n",
    "quantized_tensor_0, scale_0 = linear_q_symmetric_per_channel(test_tensor, dim=0)\n",
    "\n",
    "### along the columns (dim = 1)\n",
    "quantized_tensor_1, scale_1 = linear_q_symmetric_per_channel(test_tensor, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4beba7bca70fb8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dequantized_tensor_0 = linear_dequantization(quantized_tensor_0, scale_0, 0)\n",
    "plot_quantization_errors(test_tensor, quantized_tensor_0, dequantized_tensor_0)\n",
    "quantization_mse(dequantized_tensor_0, test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b623f3a0623f9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dequantized_tensor_1 = linear_dequantization(quantized_tensor_1, scale_1, 0)\n",
    "plot_quantization_errors(test_tensor, quantized_tensor_1, dequantized_tensor_1, n_bits=8)\n",
    "quantization_mse(dequantized_tensor_1, test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b1204013b6f128",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Per Group Quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a7e9b63d5ab378",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linear_q_symmetric_per_group(tensor, group_size, dtype=torch.int8):\n",
    "    t_shape = tensor.shape\n",
    "    assert t_shape[1] % group_size == 0\n",
    "    assert tensor.dim()==2\n",
    "    tensor = tensor.view(-1, group_size)\n",
    "    quantized_tensor, scale = linear_q_symmetric_per_channel(tensor, dim=0, dtype=dtype)\n",
    "    quantized_tensor = quantized_tensor.view(t_shape)\n",
    "    return quantized_tensor, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aecee3e77ee861",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linear_dequantization_per_group(quantized_tensor, scale, group_size):\n",
    "    q_shape = quantized_tensor.shape\n",
    "    quantized_tensor = quantized_tensor.view(-1, group_size)\n",
    "    dequantized_tensor = linear_dequantization(quantized_tensor, scale, 0)\n",
    "    dequantized_tensor = dequantized_tensor.view(q_shape)\n",
    "    return dequantized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d63f28b1777cd33",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_tensor = torch.rand((6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d6199c00672330",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group_size = 6\n",
    "quantized_tensor, scale = linear_q_symmetric_per_group(test_tensor, group_size=group_size)\n",
    "dequantized_tensor = linear_dequantization_per_group(quantized_tensor, scale, group_size=group_size)\n",
    "plot_quantization_errors(test_tensor, quantized_tensor, dequantized_tensor)\n",
    "quantization_mse(dequantized_tensor, test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8ea0bbd8c38f9d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# <font color=orange>Linear Quantization II-C: Quantizing Weights & Activations for Inference</font>\n",
    "\n",
    "In a NN we can quantize the **weights** *and* **activations** >> depending on what is quantized, the **storage** and **computation** are not the same:\n",
    "* **Storage** = Quantized Weight + Activation (e.g. W8A32) >> **Computation** = Floating Point arithmetic (FP32, FP16, BF16...)\n",
    "* **Storage** = Quantized Weight + Quantized Activation (e.g. W8A8) >> **Computation** = Integer based arithmetic (Int8, Int4...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8af84b9316a91af",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def quantized_linear_W8A32_without_bias(input, q_w, s_w, z_w):\n",
    "    assert input.dtype == torch.float32\n",
    "    assert q_w.dtype == torch.int8\n",
    "\n",
    "    dequantized_weight = q_w.to(torch.float32) * s_w + z_w\n",
    "    output = torch.nn.functional.linear(input, dequantized_weight)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1c1237004d0ae1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = torch.tensor([1,2,3], dtype=torch.float32)\n",
    "weight = torch.tensor([[-2,   -1.13, 0.42],\n",
    "                       [-1.51, 0.25, 1.62],\n",
    "                       [0.23,  1.35, 2.15]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795035207c345e03",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q_w, s_w  = linear_q_symmetric(weight)\n",
    "q_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515c27f618cb55c4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d162ade6c6d7bc8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = quantized_linear_W8A32_without_bias(input, q_w, s_w, 0)\n",
    "print(f\"This is the W8A32 output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff1b2d032c8d799",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fp32_output = torch.nn.functional.linear(input, weight)\n",
    "print(f\"This is the output if we don't quantize: {fp32_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b218a82b16731ee",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# <font color=orange>Building your own Quantizer: Custom Build an 8-Bit Quantizer</font>\n",
    "In this lesson, you will learn how to compress any model in 8-bit precision leveraging past tools. This Quantizer is Model Agnostic.\n",
    "* Create a 'W8A16LinearLayer' class to store 8-bit weights and scales\n",
    "* Replace all 'torch.nn.linear' layers with 'W8A16LinearLayer'\n",
    "* Build Quantizer and quantize the model end-to-end\n",
    "* Testing the naive absmax quantization on many scenarios and study the impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65240322c51c2e41",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645b3e33",
   "metadata": {},
   "source": [
    "### 1.1 - w8_a16_forward Function\n",
    "-\n",
    "W8A16LinearLayer\n",
    "                    # 8-bit  # 16-bit         # optional\n",
    "* w8_a16_forward -> weights, input,   scales, bias=None\n",
    "* Cast the 8-bit weights to the same data type as the input, \"casted weights\",\n",
    "* keeping the \"casted weights\" in the same range as before, [-128, 127]\n",
    "\n",
    "Next,\n",
    "\n",
    "((ùëñùëõùëùùë¢ùë°ùë† * ''casted weights'') * ùë†ùëêùëéùëôùëí) + ùëèùëñùëéùë†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5a845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_int8 = torch.randint(-128, 127, (32, 16)).to(torch.int8)\n",
    "random_hs = torch.randn((1, 16), dtype=torch.bfloat16)\n",
    "scales = torch.randn((1, 32), dtype=torch.bfloat16)\n",
    "bias = torch.randn((1, 32), dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d03767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.linear(random_hs, random_int8.to(random_hs.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c577c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.linear(random_hs, random_int8.to(random_hs.dtype)) * scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df7b37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(F.linear(random_hs, random_int8.to(random_hs.dtype)) * scales) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1be501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w8_a16_forward(weight, input, scales, bias=None):\n",
    "    \n",
    "    casted_weights = weight.to(input.dtype)\n",
    "    output = F.linear(input, casted_weights) * scales\n",
    "    \n",
    "    if bias is not None:\n",
    "        output = output + bias\n",
    "      \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9439d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With bias:\\n\\n\", \n",
    "      w8_a16_forward(random_int8, random_hs, scales, bias))\n",
    "\n",
    "print(\"\\nWithout bias:\\n\\n\", \n",
    "      w8_a16_forward(random_int8, random_hs, scales))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b7f09c",
   "metadata": {},
   "source": [
    "### 1.2 - init Function of class W8A16LinearLayer¬∂"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f6d13",
   "metadata": {},
   "source": [
    "- This is how the `init` is of [PyTorch Linear layer](https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear):\n",
    "```Python\n",
    "def __init__(self, in_features, out_features, bias=True,\n",
    "             device=None, dtype=None)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840a16c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### running this will result in an error\n",
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, \n",
    "                 bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.int8_weights = nn.Parameter(torch.Tensor([0, 1]\n",
    "                                     ).to(dtype=torch.int8))\n",
    "\n",
    "try:\n",
    "    \n",
    "    W8A16LinearLayer(1, 1)\n",
    "    \n",
    "except Exception as error:\n",
    "    print(\"\\033[91m\", type(error).__name__, \": \", error, \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9d1204",
   "metadata": {},
   "outputs": [],
   "source": [
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, \n",
    "                 bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"int8_weights\",\n",
    "            torch.randint(\n",
    "                -128, 127, (out_features, in_features), dtype=torch.int8\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.register_buffer(\"scales\", \n",
    "                             torch.randn((out_features), dtype=dtype))\n",
    "        \n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", \n",
    "                                 torch.randn((1, out_features), \n",
    "                                             dtype=dtype))\n",
    "        \n",
    "        else:\n",
    "            self.bias = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28d5773",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_instance = W8A16LinearLayer(16,32)\n",
    "print(dummy_instance.int8_weights.shape)\n",
    "print(dummy_instance.scales.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e654daa0",
   "metadata": {},
   "source": [
    "### 1.3 - `forward` Function of class `W8A16LinearLayer`\n",
    "\n",
    "- Use the `w8_a16_forward` defined earlier (Step 1.1) to define the `forward` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d4fefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, \n",
    "                 bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"int8_weights\",\n",
    "            torch.randint(\n",
    "                -128, 127, (out_features, in_features), dtype=torch.int8\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.register_buffer(\"scales\", \n",
    "                             torch.randn((out_features), dtype=dtype))\n",
    "        \n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", \n",
    "                                 torch.randn((1, out_features), \n",
    "                                             dtype=dtype))\n",
    "        \n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return w8_a16_forward(self.int8_weights, \n",
    "                              input, self.scales, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088444b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = W8A16LinearLayer(16, 32)\n",
    "dummy_hidden_states = torch.randn(1, 6, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064cdc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "module(dummy_hidden_states).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "module(dummy_hidden_states).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dee374",
   "metadata": {},
   "source": [
    "### 1.4 - `quantize` Function of class `W8A16LinearLayer`\n",
    "\n",
    "- `quantize` function will dynamically quantize half-precision weights into `torch.int8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f2ec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, \n",
    "                 bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"int8_weights\",\n",
    "            torch.randint(\n",
    "                -128, 127, (out_features, in_features), dtype=torch.int8\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.register_buffer(\"scales\", \n",
    "                             torch.randn((out_features), dtype=dtype))\n",
    "        \n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", \n",
    "                                 torch.randn((1, out_features), \n",
    "                                             dtype=dtype))\n",
    "        \n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def quantize(self, weights):\n",
    "        w_fp32 = weights.clone().to(torch.float32)\n",
    "\n",
    "        scales = w_fp32.abs().max(dim=-1).values / 127\n",
    "        scales = scales.to(weights.dtype)\n",
    "\n",
    "        int8_weights = torch.round(weights\n",
    "                        /scales.unsqueeze(1)).to(torch.int8)\n",
    "\n",
    "        self.int8_weights = int8_weights\n",
    "        self.scales = scales\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return w8_a16_forward(self.int8_weights, \n",
    "                              input, self.scales, self.bias)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a72cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = W8A16LinearLayer(4,8)\n",
    "print('Weights before:\\n' , module.int8_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2acc717",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_matrix = torch.randn((4, 8), dtype=torch.bfloat16)\n",
    "module.quantize(random_matrix)\n",
    "print(\"Weights After:\\n\" , module.int8_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e7957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "module.scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf677d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "module.scales.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d05849",
   "metadata": {},
   "outputs": [],
   "source": [
    "module.int8_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8789ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### dequantized weights\n",
    "module.int8_weights * module.scales.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f0b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "### original weights\n",
    "random_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c9bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Quantization Error\n",
    "(random_matrix - module.int8_weights \n",
    " * module.scales.unsqueeze(1)).abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b71669",
   "metadata": {},
   "source": [
    "# <font color=orange>Building your own Quantizer: Replace PyTorch layers with Quantized Layers</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba558a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fdbd52",
   "metadata": {},
   "source": [
    "### 2.1 - Model In-place Linear Layer Replacement\n",
    "- Implement `replace_linear_with_target`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a981742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_with_target(module, target_class, module_name_to_exclude):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Linear) and not any([x == name for x in module_name_to_exclude]):\n",
    "            old_bias = child.bias\n",
    "            new_module = target_class(child.in_features,\n",
    "                                      child.out_features,\n",
    "                                      old_bias is not None,\n",
    "                                      child.weight.dtype)\n",
    "            setattr(module, name, new_module) # Replace the parent module with the correct name to the new_module\n",
    "\n",
    "            if old_bias is not None:\n",
    "                getattr(module, name).bias = old_bias\n",
    "            else: \n",
    "                # Recursively call the function for nested modules\n",
    "                replace_linear_with_target(child, target_class, module_name_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b8ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.emb = torch.nn.Embedding(1, 1)\n",
    "    # Try with bias\n",
    "    self.linear_1 = nn.Linear(1, 1)\n",
    "    # Try without bias\n",
    "    self.linear_2 = nn.Linear(1, 1, bias=False)\n",
    "    # Lm prediction head\n",
    "    self.lm_head = nn.Linear(1, 1, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1040acd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = DummyModel()\n",
    "model_2 = DummyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c6ac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_linear_with_target(model_1, W8A16LinearLayer, [\"lm_head\"])\n",
    "print(model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d625103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_linear_with_target(model_2, W8A16LinearLayer, [])\n",
    "print(model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cddcb80",
   "metadata": {},
   "source": [
    "### 2.2 - Linear Layer Replacement + Quantization\n",
    "- Modify the `replace_linear_with_target` function to also perform quantization.\n",
    "- Implement `replace_linear_with_target_and_quantize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1121df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_with_target_and_quantize(module, \n",
    "                               target_class, module_name_to_exclude):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Linear) and not \\\n",
    "        any([x == name for x in module_name_to_exclude]):\n",
    "            old_bias = child.bias\n",
    "            old_weight = child.weight\n",
    "\n",
    "            new_module = target_class(child.in_features, \n",
    "                                      child.out_features, \n",
    "                                      old_bias is not None, \n",
    "                                      child.weight.dtype)\n",
    "            setattr(module, name, new_module)\n",
    "\n",
    "            getattr(module, name).quantize(old_weight)\n",
    "            \n",
    "            if old_bias is not None:\n",
    "              getattr(module, name).bias = old_bias\n",
    "        else:\n",
    "            # Recursively call the function for nested modules\n",
    "            replace_linear_with_target_and_quantize(child, \n",
    "                     target_class, module_name_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f39a261",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = DummyModel()\n",
    "replace_linear_with_target_and_quantize(model_3, W8A16LinearLayer, [\"lm_head\"])\n",
    "print(model_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52038c85",
   "metadata": {},
   "source": [
    "# <font color=orange>Building your own Quantizer: Quantize any Open Source PyTorch Model</font>\n",
    "In this lesson, you will look at the results of open source models compressed using the custom quantizer you built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a4b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f789369a",
   "metadata": {},
   "source": [
    "### Step 3: Test the Implementation on Various LLMs\n",
    "#### 3.1 - [Salesforce/codegen-350M-mono](https://huggingface.co/Salesforce/codegen-350M-mono)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cc8b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = 'Salesforce/codegen-350M-mono'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                    torch_dtype=torch.bfloat16, \n",
    "                                             low_cpu_mem_usage=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c38bfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "print(pipe(\"def hello_world():\", max_new_tokens=20, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb51388",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model before:\\n\\n\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1854f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_linear_with_target_and_quantize(model, W8A16LinearLayer, [\"lm_head\"])\n",
    "pipe.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e42789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipe(\"def hello_world():\", max_new_tokens=20, do_sample=False)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528c5d72",
   "metadata": {},
   "source": [
    "### 3.2 - [facebook/detr-resnet-50](https://huggingface.co/facebook/detr-resnet-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6225c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# you can specify the revision tag if you don't want the timm dependency\n",
    "processor = DetrImageProcessor.from_pretrained(\n",
    "    \"facebook/detr-resnet-50\", revision=\"no_timm\")\n",
    "model = DetrForObjectDetection.from_pretrained(\n",
    "    \"facebook/detr-resnet-50\", revision=\"no_timm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be3836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_memory_footprint = model.get_memory_footprint()\n",
    "previous_memory_footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fac1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"dinner_with_friends.png\"\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ca4a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "  outputs = model(**inputs)\n",
    "\n",
    "# convert outputs (bounding boxes and class logits) to COCO API\n",
    "# let's only keep detections with score > 0.9\n",
    "target_sizes = torch.tensor([image.size[::-1]])\n",
    "results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "plot_results(model, image, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827349ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bf59b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_linear_with_target_and_quantize(model, W8A16LinearLayer, [\"0\", \"1\", \"2\", \"class_labels_classifier\"])\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0412f20d",
   "metadata": {},
   "source": [
    "- Visualize results after quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e1bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "  outputs = model(**inputs)\n",
    "\n",
    "# convert outputs (bounding boxes and class logits) to COCO API\n",
    "# let's only keep detections with score > 0.9\n",
    "target_sizes = torch.tensor([image.size[::-1]])\n",
    "results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "plot_results(model, image, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_footprint = model.get_memory_footprint()\n",
    "print(\"Footprint of the model in MBs: \", new_footprint/1e+6)\n",
    "### Memory saved\n",
    "print(\"Memory saved in MBs: \", (previous_memory_footprint - new_footprint)/1e+6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c6af12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
